Weighted K-NN: This ia a modified version of K-Nearest Neughbors.
The main drawback of k-NN algorithm is the choice of the value of
the hyperparameter 'k'. If k is too small, the algorithm would be 
more sensitive to outliers. Again if k is too large, then the 
neighborhood may include too many points from other classes.

Example:
Class-A: ((0,4),(1,4.9),(1.6,5.4),(2.8,7),(3.2,8),(3.4,9))
Class-B:((1.8,1),(2.2,3),(3,4),(4,4.5),(5,5),(6,5.5))

New-Data Item: (2,4)
k=5


Old_X	Old_Y	New_X	New_Y	Distance	Class		
0	4	2	4	2		A		
1	4.9	2	4	1.35		A		
1.6	5.4	2	4	1.46		A		
2.8	7	2	4	3.1			
3.2	8	2	4	4.18			For K=5
3.4	9	2	4	5.19			
1.8	1	2	4	3.01			
2.2	3	2	4	1.02		B		
3	4	2	4	1		B		
4	4.5	2	4	2.06			
5	5	2	4	3.16			
6	5.5	2	4	4.27			


If we give the above dataset to a k-NN classifier, then it assigns
the new data item into Class-A. But if we plot all the data points,
it will be clear that the new data item is more closer to the 
Class-B compared to Class-A.

To overcome this disadvantage, weighted K-NN is used. In weighted
K-NN, the nearest k neighbors are given a weight using a function.
The intuition behind weighted K-NN is to give more weight to the
points which are nearby and less weight to the points which are
farther away. Any function can be used for the weighted K-NN whose
value decreases as the distance increases. The simple function which
is used is the inverse distance function.